---
layout: post
title: Training and Vizualizing Self Organizing Map(SOM) with Numerical Data
---

> Recently i have started working in a cognitive science project. In our project, we wanted to use Self organising Map(SOM) for some cluster 
>analysis purpose. While doing some research and implementation with SOM, i felt i could share my experience and learning with everyone and i 
>can discuss step by step. I have implemented SOM with numerical dataset, the dataset is not so big either small. I will present and discuss 
>the result a bit.
>Note: Data is not public , so not possible to share.  

## *Let's Talk about SOM a bit....*

A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.
This makes SOMs useful for visualization by creating low-dimensional views of high-dimensional data, akin to multidimensional scaling.

![SOM](/images/SOmDR.png)


>The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network.

## SOM Algorithm:

Each data from data set recognizes themselves by competeting for representation. SOM mapping steps starts from initializing the weight vectors. From there a sample vector is selected randomly and the map of weight vectors is searched to find which weight best represents that sample. Each weight vector has neighboring weights that are close to it. The weight that is chosen is rewarded by being able to become more like that randomly selected sample vector. The neighbors of that weight are also rewarded by being able to become more like the chosen sample vector. From this step the number of neighbors and how much each weight can learn decreases over time. This whole process is repeated a large number of times, usually more than 1000 times. 

In sum, learning occurs in several steps and over many iterations. :

>1. Each node's weights are initialized.
>2. A vector is chosen at random from the set of training data.
>3. Every node is examined to calculate which one's weights are most like the input vector. The winning node is commonly known as the Best Matching Unit (BMU).
>4. Then the neighbourhood of the BMU is calculated. The amount of neighbors decreases over time.
>5. The winning weight is rewarded with becoming more like the sample vector. The nighbors also become more like the sample vector. The closer a node is to the BMU, the more its weights get altered and the farther away the neighbor is from the BMU, the less it learns.
>6. Repeat step 2 for N iterations.

Best Matching Unit is a technique which calculates the distance from each weight to the sample vector, by running through all weight vectors. The weight with the shortest distance is the winner. There are numerous ways to determine the distance, however, the most commonly used method is the Euclidean Distance, and thatâ€™s what is used in the following implementation